{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80932ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Anas\n",
      "[nltk_data]     Laptop\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import spacy\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e399a5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train1.csv')\n",
    "test = pd.read_csv('test1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7689beb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train.head(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2ec76db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wed Jun 03 03:47:24 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mayhemstudios</td>\n",
       "      <td>@JennaMadison awww, thanks . I like you too, a...</td>\n",
       "      <td>1325379</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mon Jun 01 05:54:18 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ruralmama</td>\n",
       "      <td>&amp;quot;The man&amp;quot; is gone for a week. Pouting!</td>\n",
       "      <td>275675</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wed Jun 17 17:34:38 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>princesslaylah</td>\n",
       "      <td>stupid itunes keeps crashing so i can't update...</td>\n",
       "      <td>583702</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fri May 29 11:43:58 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>candace_lea</td>\n",
       "      <td>is ready for the weekend!</td>\n",
       "      <td>1058019</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mon Jun 15 12:45:11 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joluvs2shop</td>\n",
       "      <td>@schofe must be great getting all the freebies...</td>\n",
       "      <td>1545430</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           date      flag            user  \\\n",
       "0  Wed Jun 03 03:47:24 PDT 2009  NO_QUERY   mayhemstudios   \n",
       "1  Mon Jun 01 05:54:18 PDT 2009  NO_QUERY       ruralmama   \n",
       "2  Wed Jun 17 17:34:38 PDT 2009  NO_QUERY  princesslaylah   \n",
       "3  Fri May 29 11:43:58 PDT 2009  NO_QUERY     candace_lea   \n",
       "4  Mon Jun 15 12:45:11 PDT 2009  NO_QUERY     joluvs2shop   \n",
       "\n",
       "                                                text       id  target  \n",
       "0  @JennaMadison awww, thanks . I like you too, a...  1325379       1  \n",
       "1  &quot;The man&quot; is gone for a week. Pouting!    275675       0  \n",
       "2  stupid itunes keeps crashing so i can't update...   583702       0  \n",
       "3                         is ready for the weekend!   1058019       1  \n",
       "4  @schofe must be great getting all the freebies...  1545430       1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f563313d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 6)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb4ee566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af0577b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anas Laptop\\AppData\\Local\\Temp\\ipykernel_23788\\3527098657.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.drop(['date','flag','user'],axis=1 ,inplace=True)\n"
     ]
    }
   ],
   "source": [
    "df.drop(['date','flag','user'],axis=1 ,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97380b82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@JennaMadison awww, thanks . I like you too, a...</td>\n",
       "      <td>1325379</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&amp;quot;The man&amp;quot; is gone for a week. Pouting!</td>\n",
       "      <td>275675</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stupid itunes keeps crashing so i can't update...</td>\n",
       "      <td>583702</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>is ready for the weekend!</td>\n",
       "      <td>1058019</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@schofe must be great getting all the freebies...</td>\n",
       "      <td>1545430</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text       id  target\n",
       "0  @JennaMadison awww, thanks . I like you too, a...  1325379       1\n",
       "1  &quot;The man&quot; is gone for a week. Pouting!    275675       0\n",
       "2  stupid itunes keeps crashing so i can't update...   583702       0\n",
       "3                         is ready for the weekend!   1058019       1\n",
       "4  @schofe must be great getting all the freebies...  1545430       1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69ce38b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anas Laptop\\AppData\\Local\\Temp\\ipykernel_23788\\3384779416.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text'] = df['text'].apply(lambda x: re.sub(url_pattern, 'URL', x))\n",
      "C:\\Users\\Anas Laptop\\AppData\\Local\\Temp\\ipykernel_23788\\3384779416.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text'] = df['text'].apply(lambda x: re.sub(user_pattern, 'USER', x))\n",
      "C:\\Users\\Anas Laptop\\AppData\\Local\\Temp\\ipykernel_23788\\3384779416.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text'] = df['text'].apply(lambda x: re.sub(alpha_pattern, \"\", x))\n",
      "C:\\Users\\Anas Laptop\\AppData\\Local\\Temp\\ipykernel_23788\\3384779416.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text'] = df['text'].apply(lambda x: re.sub(sequence_pattern, seqReplace_pattern, x))\n",
      "C:\\Users\\Anas Laptop\\AppData\\Local\\Temp\\ipykernel_23788\\3384779416.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text'] = df['text'].apply(lambda x: x.lower())\n"
     ]
    }
   ],
   "source": [
    "url_pattern = re.compile(r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\")\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(url_pattern, 'URL', x))\n",
    "\n",
    "user_pattern = re.compile(r'@[\\w]+')\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(user_pattern, 'USER', x))\n",
    "\n",
    "# The Thrid step convert any Emoticon keyboard shortcuts to Words\n",
    "emojis = {'(:': 'Smiley face', '(-:': 'Smiley face', '[:': 'Content face',\n",
    "         '(;': 'Winky face', '(-;': 'Winky face', 'D:': 'Thrilled face', 'D-:': 'Thrilled face',\n",
    "         'P;': 'Goofy face', 'P-;': 'Goofy face', 'P:': 'Silly face', 'P-:': 'Silly face',\n",
    "         '(8': 'Cool guy face', '(-8': 'Cool guy face', '|:':'Blank face', '|-:': 'Blank face',\n",
    "         '(:': 'Sad face', '(-:':'Sad face', 'O_o':'Grossed out face', 'O.o':'Grossed out face',\n",
    "         '\\:': 'Sick face', '\\-:': 'Sick face', 'O:': 'Surprised face', 'O-:': 'Surprised face',\n",
    "         'O_O': 'Tweak face', 'O.O': 'Tweak face', 'o_o': 'Grossed out face', '(#':'Poundie',\n",
    "         '(-#': 'Poundie', '^.^': 'Nerdie Poundie', '^_^':'Nerdie Poundie', '^^':'Nerdie Poundie',\n",
    "         'x:': 'Kissy face', 'x-:': 'Kissy face','<3': 'Heart', ')#': 'Frowndie', ')-#': 'Frowndie',\n",
    "         '(m': 'Facepalm', '(-m': 'Facepalm', 'Dx': 'Crying laughing', 'DX': 'Crying laughing',\n",
    "         \")':\": 'Crying face', \")-':\": 'Crying face',':)': 'smile', ':-)': 'smile', ';d': 'wink', ':-E': 'vampire', ':(': 'sad',\n",
    "          ':-(': 'sad', ':-<': 'sad', ':P': 'raspberry', ':O': 'surprised',\n",
    "          ':-@': 'shocked', ':@': 'shocked',':-$': 'confused', ':\\\\': 'annoyed',\n",
    "          ':#': 'mute', ':X': 'mute', ':^)': 'smile', ':-&': 'confused', '$_$': 'greedy',\n",
    "          '@@': 'eyeroll', ':-!': 'confused', ':-D': 'smile', ':-0': 'yell', 'O.o': 'confused',\n",
    "          '<(-_-)>': 'robot', 'd[-_-]b': 'dj', \":'-)\": 'sadsmile', ';)': 'wink',\n",
    "          ';-)': 'wink', 'O:-)': 'angel','O*-)': 'angel','(:-D': 'gossip', '=^.^=': 'cat'}\n",
    "\n",
    "for emoji in emojis.keys():\n",
    "    df['text'].replace(emoji,'EMOJI'+emojis[emoji])\n",
    "    \n",
    "# The Fourth step removing Non-Alphabets\n",
    "alpha_pattern = \"[^a-zA-Z0-9\\s]\"\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(alpha_pattern, \"\", x))\n",
    "\n",
    "# The Fivth step removing Consecutive letters than more 3\n",
    "\n",
    "sequence_pattern   = r\"(.)\\1\\1+\"\n",
    "seqReplace_pattern = r\"\\1\\1\"\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(sequence_pattern, seqReplace_pattern, x))\n",
    "\n",
    "\n",
    "# The Sixth step convert the text to lower case\n",
    "\n",
    "df['text'] = df['text'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a530db10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anas Laptop\\AppData\\Local\\Temp\\ipykernel_23788\\289386078.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['cleaned_text'] = df['text'].apply(clean_tokenize)\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to clean and tokenize text\n",
    "def clean_tokenize(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Remove usernames\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^A-Za-z ]+', '', text)\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stop words+\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df['cleaned_text'] = df['text'].apply(clean_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59f017b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                        user aww thanks like always funny\n",
       "1                        quotthe manquot gone week pouting\n",
       "2        stupid itunes keeps crashing cant update iphon...\n",
       "3                                            ready weekend\n",
       "4        user must great getting freebies dont want im ...\n",
       "                               ...                        \n",
       "99995             boy supposed love love friend soo angryy\n",
       "99996    user paid hour fair yes even mopped kitchen fl...\n",
       "99997              user whoo scrubs borrow six season body\n",
       "99998       great day rampr glad sun makes things brighter\n",
       "99999                            mowing lawn weather sucks\n",
       "Name: cleaned_text, Length: 100000, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cleaned_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c0c686e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>user aww thanks  i like you too always funny</td>\n",
       "      <td>1325379</td>\n",
       "      <td>1</td>\n",
       "      <td>user aww thanks like always funny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>quotthe manquot is gone for a week pouting</td>\n",
       "      <td>275675</td>\n",
       "      <td>0</td>\n",
       "      <td>quotthe manquot gone week pouting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stupid itunes keeps crashing so i cant update ...</td>\n",
       "      <td>583702</td>\n",
       "      <td>0</td>\n",
       "      <td>stupid itunes keeps crashing cant update iphon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>is ready for the weekend</td>\n",
       "      <td>1058019</td>\n",
       "      <td>1</td>\n",
       "      <td>ready weekend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>user must be great getting all the freebies an...</td>\n",
       "      <td>1545430</td>\n",
       "      <td>1</td>\n",
       "      <td>user must great getting freebies dont want im ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text       id  target  \\\n",
       "0      user aww thanks  i like you too always funny   1325379       1   \n",
       "1        quotthe manquot is gone for a week pouting    275675       0   \n",
       "2  stupid itunes keeps crashing so i cant update ...   583702       0   \n",
       "3                          is ready for the weekend   1058019       1   \n",
       "4  user must be great getting all the freebies an...  1545430       1   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0                  user aww thanks like always funny  \n",
       "1                  quotthe manquot gone week pouting  \n",
       "2  stupid itunes keeps crashing cant update iphon...  \n",
       "3                                      ready weekend  \n",
       "4  user must great getting freebies dont want im ...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78c748ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anas Laptop\\AppData\\Local\\Temp\\ipykernel_23788\\287628158.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['cleaned_text'] = df['text'].apply(stem_text)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "# Initialize PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# Define a function to stem each word in a text\n",
    "def stem_text(text):\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "    # Stem each word\n",
    "    stemmed_words = [ps.stem(word) for word in words]\n",
    "    # Join the stemmed words back into a single string\n",
    "    return ' '.join(stemmed_words)\n",
    "\n",
    "# Assuming 'df' is your DataFrame and 'text' is the column with text data\n",
    "# Apply the function to each row in the 'text' column\n",
    "df['cleaned_text'] = df['text'].apply(stem_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3989882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>user aww thanks  i like you too always funny</td>\n",
       "      <td>1325379</td>\n",
       "      <td>1</td>\n",
       "      <td>user aww thank i like you too alway funni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>quotthe manquot is gone for a week pouting</td>\n",
       "      <td>275675</td>\n",
       "      <td>0</td>\n",
       "      <td>quotth manquot is gone for a week pout</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stupid itunes keeps crashing so i cant update ...</td>\n",
       "      <td>583702</td>\n",
       "      <td>0</td>\n",
       "      <td>stupid itun keep crash so i cant updat my ipho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>is ready for the weekend</td>\n",
       "      <td>1058019</td>\n",
       "      <td>1</td>\n",
       "      <td>is readi for the weekend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>user must be great getting all the freebies an...</td>\n",
       "      <td>1545430</td>\n",
       "      <td>1</td>\n",
       "      <td>user must be great get all the freebi ani you ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text       id  target  \\\n",
       "0      user aww thanks  i like you too always funny   1325379       1   \n",
       "1        quotthe manquot is gone for a week pouting    275675       0   \n",
       "2  stupid itunes keeps crashing so i cant update ...   583702       0   \n",
       "3                          is ready for the weekend   1058019       1   \n",
       "4  user must be great getting all the freebies an...  1545430       1   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0          user aww thank i like you too alway funni  \n",
       "1             quotth manquot is gone for a week pout  \n",
       "2  stupid itun keep crash so i cant updat my ipho...  \n",
       "3                           is readi for the weekend  \n",
       "4  user must be great get all the freebi ani you ...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5deae0e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            user aww thank i like you too alway funni\n",
       "1               quotth manquot is gone for a week pout\n",
       "2    stupid itun keep crash so i cant updat my ipho...\n",
       "3                             is readi for the weekend\n",
       "4    user must be great get all the freebi ani you ...\n",
       "Name: cleaned_text, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cleaned_text'][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dea19930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Anas Laptop\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "\n",
    "#max_features = 2000\n",
    "vectorizer = CountVectorizer(max_features=None)\n",
    "text = csr_matrix(vectorizer.fit_transform(df['cleaned_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea9799d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8b3806",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a18f67e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #vectorizer = TfidfVectorizer()\n",
    "# X = vectorizer.fit_transform(df['text'])\n",
    "# y = df['target']\n",
    "\n",
    "# # Splitting the data\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "# # Model\n",
    "# model = LogisticRegression()\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# # Predictions\n",
    "# predictions = model.predict(X_test)\n",
    "\n",
    "# # Evaluation\n",
    "# print(classification_report(y_test, predictions))\n",
    "\n",
    "# #####################\n",
    "# # vectorizer = TfidfVectorizer(max_features=None)\n",
    "# # X = vectorizer.fit_transform(df['text'])\n",
    "# # y = df['target']\n",
    "\n",
    "# # # Split the data\n",
    "# # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# # # XGBoost model\n",
    "# # xgb_model = XGBClassifier()\n",
    "# # xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# # # Predictions\n",
    "# # predictions = xgb_model.predict(X_test)\n",
    "\n",
    "# # # Evaluation\n",
    "# # print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "95111ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = vectorizer.fit_transform(df['cleaned_text'])\n",
    "# y = df['target']\n",
    "\n",
    "# # Splitting the data\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "# # Model\n",
    "# model = LogisticRegression()\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# # Predictions\n",
    "# predictions = model.predict(X_test)\n",
    "\n",
    "# # Evaluation\n",
    "# print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0998c843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #vectorizer = TfidfVectorizer()\n",
    "# vectorizer = TfidfVectorizer(max_features=None)\n",
    "\n",
    "# X = vectorizer.fit_transform(df['text'])\n",
    "# y = df['target']\n",
    "\n",
    "# # Splitting the data\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "# # Model\n",
    "# model = LogisticRegression()\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# # Predictions\n",
    "# predictions = model.predict(X_test)\n",
    "\n",
    "# # Evaluation\n",
    "# print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13944e25",
   "metadata": {},
   "source": [
    "Logistic Reg  \n",
    "1- Count\n",
    "2-tdid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b6d87db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = TfidfVectorizer(max_features=None)\n",
    "# X = vectorizer.fit_transform(df['text'])\n",
    "# y = df['target']\n",
    "\n",
    "# # Split the data\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# # XGBoost model\n",
    "# xgb_model = XGBClassifier()\n",
    "# xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# # Predictions\n",
    "# predictions = xgb_model.predict(X_test)\n",
    "\n",
    "# # Evaluation\n",
    "# print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "26657e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = CountVectorizer(max_features=None)\n",
    "# X = vectorizer.fit_transform(df['text'])\n",
    "# y = df['target']\n",
    "\n",
    "# # Split the data\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# # XGBoost model\n",
    "# xgb_model = XGBClassifier()\n",
    "# xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# # Predictions\n",
    "# predictions = xgb_model.predict(X_test)\n",
    "\n",
    "# # Evaluation\n",
    "# print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7adcbe94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = CountVectorizer(max_features=None)\n",
    "# X = vectorizer.fit_transform(df['text'])\n",
    "# y = df['target']\n",
    "\n",
    "# # Split the data\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# # XGBoost model\n",
    "# xgb_model = xgb.XGBClassifier(n_estimators = 5000,  random_state = 42 , max_depth  = 10 ,learning_rate = 0.2\n",
    "#                            ,colsample_bytree =0.8,subsample = 1 , min_child_weight =10)\n",
    "# xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# # Predictions\n",
    "# predictions = xgb_model.predict(X_test)\n",
    "\n",
    "# # Evaluation\n",
    "# print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "82d8dd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = CountVectorizer(max_features=None)\n",
    "# X = vectorizer.fit_transform(df['cleaned_text'])\n",
    "# y = df['target']\n",
    "\n",
    "# # Split the data\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# # XGBoost model\n",
    "# xgb_model = xgb.XGBClassifier(n_estimators = 5000,  random_state = 42 , max_depth  = 10 ,learning_rate = 0.2\n",
    "#                            ,colsample_bytree =0.8,subsample = 1 , min_child_weight =10)\n",
    "# xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# # Predictions\n",
    "# predictions = xgb_model.predict(X_test)\n",
    "\n",
    "# # Evaluation\n",
    "# print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e44e18f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Assuming you have new data in a DataFrame called new_data\n",
    "# new_text_data = test['text']\n",
    "\n",
    "# new_X = vectorizer.transform(new_text_data)\n",
    "\n",
    "\n",
    "# new_predictions = xgb_model.predict(new_X)\n",
    "\n",
    "# # Add a new column 'predicted_target' to test\n",
    "# #test['predicted_target'] = new_predictions\n",
    "\n",
    "# # Add a new column 'predicted_target' to new_data\n",
    "# test['target'] = new_predictions\n",
    "\n",
    "# # Save the DataFrame to a CSV file\n",
    "# #test.to_csv('predicted_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7ae26ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.read_csv('sample.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d222112c",
   "metadata": {},
   "source": [
    "Xgbost \n",
    "1- tFid \n",
    "2- Counter\n",
    "3-Counter with HP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e690365",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample['target'] = new_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5a581e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a967667",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample.to_csv('sample010.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab6c5db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.76      0.77     14827\n",
      "           1       0.77      0.78      0.78     15173\n",
      "\n",
      "    accuracy                           0.77     30000\n",
      "   macro avg       0.77      0.77      0.77     30000\n",
      "weighted avg       0.77      0.77      0.77     30000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anas Laptop\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(max_features=None)\n",
    "text = csr_matrix(vectorizer.fit_transform(df['cleaned_text']))\n",
    "X = vectorizer.fit_transform(df['cleaned_text'])\n",
    "y = df['target']\n",
    "\n",
    "# Splitting the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# Model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "360b690d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.74      0.73     15017\n",
      "           1       0.73      0.73      0.73     14983\n",
      "\n",
      "    accuracy                           0.73     30000\n",
      "   macro avg       0.73      0.73      0.73     30000\n",
      "weighted avg       0.73      0.73      0.73     30000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anas Laptop\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Tokenize your text (assuming df['cleaned_text'] is already a list of words for each document)\n",
    "tokenized_sentences = [text.split() for text in df['cleaned_text']]\n",
    "\n",
    "# Train Word2Vec\n",
    "vectorizer = gensim.models.Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Function to create averaged word vector for a document\n",
    "def document_vector(doc):\n",
    "    # Remove out-of-vocabulary words\n",
    "    doc = [word for word in doc if word in vectorizer.wv.index_to_key]\n",
    "    return np.mean(vectorizer.wv[doc], axis=0) if len(doc) > 0 else np.zeros(vectorizer.vector_size)\n",
    "\n",
    "# Create feature vectors for each document\n",
    "X = np.array([document_vector(doc) for doc in tokenized_sentences])\n",
    "\n",
    "y = df['target']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# Model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bb871089",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming you have new data in a DataFrame called new_data\n",
    "new_text_data = test['text']\n",
    "\n",
    "new_X = vectorizer.transform(new_text_data)\n",
    "\n",
    "\n",
    "new_predictions = model.predict(new_X)\n",
    "\n",
    "# Add a new column 'predicted_target' to test\n",
    "#test['predicted_target'] = new_predictions\n",
    "\n",
    "# Add a new column 'predicted_target' to new_data\n",
    "test['target'] = new_predictions\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "#test.to_csv('predicted_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2cbc08db",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample['target'] = new_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c70c5742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         1\n",
       "1         1\n",
       "2         1\n",
       "3         1\n",
       "4         0\n",
       "         ..\n",
       "255995    0\n",
       "255996    1\n",
       "255997    0\n",
       "255998    0\n",
       "255999    0\n",
       "Name: target, Length: 256000, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b2f7cad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.to_csv('sample020.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "153ee5a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector representation of 'books': [-8.6196596e-03  3.6659390e-03  5.1903180e-03  5.7418076e-03\n",
      "  7.4669695e-03 -6.1675780e-03  1.1056825e-03  6.0472889e-03\n",
      " -2.8401462e-03 -6.1733695e-03 -4.1022216e-04 -8.3692092e-03\n",
      " -5.5998950e-03  7.1045891e-03  3.3525999e-03  7.2255637e-03\n",
      "  6.7998939e-03  7.5310115e-03 -3.7893557e-03 -5.6184293e-04\n",
      "  2.3484002e-03 -4.5189029e-03  8.3890324e-03 -9.8582022e-03\n",
      "  6.7643207e-03  2.9147104e-03 -4.9326117e-03  4.3977732e-03\n",
      " -1.7394728e-03  6.7113303e-03  9.9646291e-03 -4.3627555e-03\n",
      " -5.9921597e-04 -5.6956350e-03  3.8510798e-03  2.7867644e-03\n",
      "  6.8911952e-03  6.1009317e-03  9.5382128e-03  9.2731910e-03\n",
      "  7.8979973e-03 -6.9897701e-03 -9.1560259e-03 -3.5538655e-04\n",
      " -3.0997922e-03  7.8942655e-03  5.9384434e-03 -1.5459724e-03\n",
      "  1.5108237e-03  1.7897255e-03  7.8173140e-03 -9.5105413e-03\n",
      " -2.0546034e-04  3.4690993e-03 -9.3898363e-04  8.3820624e-03\n",
      "  9.0108793e-03  6.5364111e-03 -7.1155117e-04  7.7104783e-03\n",
      " -8.5343765e-03  3.2068116e-03 -4.6381308e-03 -5.0888825e-03\n",
      "  3.5896117e-03  5.3704837e-03  7.7693309e-03 -5.7660667e-03\n",
      "  7.4333642e-03  6.6257357e-03 -3.7099514e-03 -8.7455688e-03\n",
      "  5.4376815e-03  6.5101348e-03 -7.8756665e-04 -6.7096334e-03\n",
      " -7.0859096e-03 -2.4972779e-03  5.1430026e-03 -3.6653224e-03\n",
      " -9.3701892e-03  3.8266983e-03  4.8845485e-03 -6.4286878e-03\n",
      "  1.2082956e-03 -2.0745820e-03  2.4383771e-05 -9.8837474e-03\n",
      "  2.6919260e-03 -4.7500827e-03  1.0879493e-03 -1.5763132e-03\n",
      "  2.1966216e-03 -7.8817401e-03 -2.7175178e-03  2.6636641e-03\n",
      "  5.3470316e-03 -2.3915586e-03 -9.5102219e-03  4.5057507e-03]\n",
      "\n",
      "Words similar to 'books': [('are', 0.1607232540845871), ('fun', 0.1372511088848114), ('great', 0.12303002178668976), ('enjoy', 0.06797570735216141), ('learning', 0.03365381807088852)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Anas\n",
      "[nltk_data]     Laptop\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Sample sentences\n",
    "sentences = [\"I enjoy reading books\",\n",
    "             \"I love writing code\",\n",
    "             \"I read a lot\",\n",
    "             \"Programming is fun\",\n",
    "             \"Books are great for learning\"]\n",
    "\n",
    "# Tokenizing the sentences\n",
    "nltk.download('punkt')\n",
    "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "\n",
    "# Training the Word2Vec model\n",
    "model = Word2Vec(tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Getting the word vector for a specific word\n",
    "word_vector = model.wv['books']\n",
    "\n",
    "# Finding similar words\n",
    "similar_words = model.wv.most_similar('books', topn=5)\n",
    "\n",
    "print(\"Vector representation of 'books':\", word_vector)\n",
    "print(\"\\nWords similar to 'books':\", similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74694c97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
